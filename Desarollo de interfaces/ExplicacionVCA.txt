Visión general del proyecto

El proyecto implementa una interfaz natural de usuario basada en:

Voz (reconocimiento + síntesis) para ejecutar comandos.

Cámara (MediaPipe Hands) para detectar manos/gestos (si hay webcam).

Realidad aumentada (AR.js + A-Frame) para mostrar un objeto 3D sobre un marcador (si hay webcam).

Toda la lógica se construye con una arquitectura modular: cada tecnología vive en su módulo, y se conectan a través de un estado común.

index.html — UI + contenedores + carga de módulos

Responsabilidad: es la “carcasa” del sistema.

Incluye:

La barra superior (topbar) con botones:

Escuchar (micro)

Manos ON/OFF

AR ON/OFF

Parar cámara

Panel de estado:

Último texto reconocido

Última operación y parámetro

Último gesto detectado

Modo de cámara activo

Tabla de clientes (demo) para ver el efecto de comandos CRUD/edición.

Dos “stages”:

MediaPipe: <video> oculto + <canvas> visible

AR: contenedor donde se monta/desmonta el <a-scene>

Además carga librerías por CDN:

MediaPipe Hands

A-Frame

AR.js

Y carga tus scripts en este orden (importante):

state.js (estado global)

voice.js (voz)

mediapipe-hands.js (gestos)

ar-scene.js (AR)

app.js (arranque y UI)

css/styles.css — Tema naranja/negro + componentes visuales

Responsabilidad: define estilo global y componentes UI.

Claves del CSS:

Variables en :root con paleta negro + naranja.

Fondo con gradientes radiales para dar un look “tech”.

Botones con hover glow naranja.

Paneles con degradado oscuro.

Tabla clientes con bordes y cabecera naranja.

Funcionalmente no afecta a lógica, pero sí mejora la presentación y la claridad al usar el proyecto.

js/state.js — Estado global + “acciones” del sistema

Este archivo es el núcleo del proyecto.

1) Estado centralizado

Mantiene un objeto state con:

voice: último transcript, operación, parámetro, y si voz está habilitada.

hands: si está activo y último gesto.

ar: si está activo, color y visibilidad del objeto.

cameraMode: "NONE" | "HANDS" | "AR" (para saber qué módulo tiene la cámara).

clientes: array de objetos (nombre, apellidos, email).

doc: simula un documento abierto/cambios (demo para comandos abrir/guardar/cerrar).

2) Sincronización de UI (ui.sync)

Cada vez que cambia algo en el estado:

actualiza textos del panel (“Última voz”, “Operación”, etc.)

actualiza el texto de botones (“Manos: ON/OFF”)

repinta la tabla de clientes

Esto te da una ventaja: no tienes que “acordarte” de refrescar la interfaz en cada acción; lo hace el estado.

3) Funciones de datos (funcionalidades)

Aquí es donde se implementa la parte de “microERP”:

insertarClienteDemo(): añade un cliente nuevo.

eliminarClientePorIndice(i): borra por índice.

actualizarClienteDemo(i): modifica el nombre añadiendo “(OK)”.

listarClientes(): devuelve el array (útil para debug).

buscarClientePorNombre(keyword): filtra por nombre/apellidos/email.

editarClienteCampo(i, campo, valor) (la funcionalidad nueva clave): permite editar cualquier campo por voz con validación.

Y también funciones de “documento”:

abrirDocumento, cerrarDocumento, guardarDocumento, crearDocumento, reiniciarDemo.

4) Funciones puente hacia AR

arSetVisible(true/false) y arSetColor(color) actualizan estado y llaman a ARScene para reflejarlo en el objeto 3D si AR está activo.

js/voice.js — Reconocimiento y comandos por voz (tu estilo)

Este módulo implementa:

1) Reconocimiento de voz

Usa:

SpeechRecognition / webkitSpeechRecognition

idioma es-ES

escucha bajo demanda al pulsar “Escuchar”

Cuando recibe resultado:

guarda el texto (setTranscript)

extrae:

operacion = primera palabra

parametro = segunda palabra

y ejecuta switch(operacion)

Esto reproduce exactamente tu patrón original.

2) Síntesis de voz

Con SpeechSynthesisUtterance:

busca voz española si existe

“habla” respuestas (feedback al usuario)

3) Normalización y utilidades

clean() limpia signos y pasa a minúsculas.

palabraANumero() convierte “uno/dos/tres” a índices.

colorNameToHex() convierte “rojo/verde/…” a HEX.

normalizarOperacion() convierte sinónimos a un comando real:

“borra” → “eliminar”

“muestra” → “mostrar”

“modificar” → “editar”
etc.

4) Switch de comandos (funcionalidades)

El switch(operacion) soporta:

CRUD:

leer, listar, insertar, actualizar, eliminar

Documento demo:

crear, abrir, guardar, cerrar

Búsqueda:

buscar <palabra>

Módulos:

activar manos/ar

desactivar manos/ar

AR:

mostrar cubo, ocultar cubo, color rojo

Utilidad:

ayuda, parar/stop, reiniciar/reset, repetir

5) Funcionalidad añadida clave: edición por voz real

Con el comando editar se parsea la frase completa, no solo la segunda palabra.

Ejemplo:

editar uno nombre Miguel Ángel

Interpretación:

operación = editar

índice = uno → 1

campo = nombre

valor = resto de palabras → “Miguel Ángel”

Esto permite edición real de registros por voz.

js/app.js — Arranque + control de cámara y botones

Este archivo conecta UI con módulos.

1) Inicialización

En DOMContentLoaded llama a App.init():

sincroniza UI

engancha eventos

inicializa voz

2) Control de cámara exclusivo

Muy importante: MediaPipe y AR usan webcam. Para evitar conflictos, app.js implementa exclusión:

Si activas Manos, desactiva AR

Si activas AR, desactiva Manos

Funciones:

enableHands(true/false)

enableAR(true/false)

disableAllCamera() para el botón “Parar”

Esto evita errores de “cámara ocupada” y hace la demo estable.

js/mediapipe-hands.js — Cámara + detección de manos (si hay webcam)

Este módulo:

pide getUserMedia({video:true})

usa MediaPipe Hands para detectar landmarks

dibuja la cámara y los puntos en canvas

También incluye un gesto simple:

mano abierta / puño (según distancias)

Y opcionalmente puede mapear gesto a acción (por ejemplo mostrar/ocultar), pero el proyecto ya funciona incluso sin webcam.

js/ar-scene.js — Montaje AR dinámico (si hay webcam)

Este archivo:

construye el HTML del <a-scene>

lo monta dentro del contenedor #arMount cuando activas AR

lo desmonta cuando lo desactivas (liberando cámara)

Dentro del marcador HIRO:

hay un cubo animado (a-box) con rotación

funciones para:

cambiar visibilidad

cambiar color

Funcionalidades añadidas respecto al proyecto base
1) Edición real de datos por voz (principal)

Antes: comandos simples de control o CRUD básico.
Ahora: editar campos completos de un cliente con valor libre por frase completa.

2) Parser más completo manteniendo tu estilo

Se mantiene switch(operacion), pero ahora:

hay normalización de sinónimos

se interpreta frase completa en comandos que lo necesitan (editar, etc.)

3) Arquitectura modular + estado central

Separación clara de responsabilidades y actualización de UI automática mediante state.ui.sync().

4) Robustez y estabilidad

Control exclusivo de cámara

Manejo de errores y estados

Proyecto usable incluso sin webcam

5) Rebranding visual (naranja/negro)

Tema coherente, botones con glow, mejor legibilidad, apariencia más “producto”.

Frase final para presentar el proyecto

Este proyecto implementa una interfaz natural basada en voz (y opcionalmente cámara y RA), donde los comandos de voz se interpretan mediante un parser estructurado (operación+parámetro) y se traducen a acciones sobre un estado centralizado que actualiza automáticamente la interfaz y los datos de aplicación, incluyendo edición dinámica de registros por voz.